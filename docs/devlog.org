* Development Log
** 2023-11-27
*** Adding more projects
So far, running projects in docker containers seems easy enough, and I have not spent very much time at all tracking down dependency issues. In fact, since all of my setup steps were written down, every time I run into something unexpected, it's extremely easy for me to retrace my steps and diagnose what's going wrong.

I was able to diagnose a dependency-related issue and suggest a fix in a fastai notebook: https://github.com/fastai/diffusion-nbs/pull/45
And the diffusers library: (linked from above)

With my isolated environment.

*** Automatic1111/StableDiffusionWebUI
Fun with Stable Diffusion
*** x11nvc
Remote control of X desktop apps.

Hard to get a web browser running inside of a docker container without issue. Hmm.

*** Memory Cache
**** privateGPT
Minor build issue resolved with a hack in the dockerfile:

#+begin_src
RUN poetry run pip3 install transformers[torch]
#+end_src

Perhaps the project needed an older version of cuda? It seems that way based on the output.


#+begin_src
 => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 849.3/849.3 kB 7.9 MB/s eta 0:00:00
 => => # Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch!=1.12.0,>=1.10->transformers[torch])
 => => #   Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)
 => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.8/11.8 MB 7.3 MB/s eta 0:00:00
 => => # Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch!=1.12.0,>=1.10->transformers[torch])
 => => #   Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)
#+end_src

This fix worked, but I'm not sure why or how the dependency is / was supposed to be tracked.

**** Firefox
For memory cache, I need to build a custom version of Firefox.

I'd like to try just building a firefox extension, but after an hour of fiddling and failing to get jsPDF loaded in my extension properly I just decided to follow Liv's guide.

The guide says:

#+begin_src
Modify tabs.json to add a new property in the printerSettings JSON properties:
...
For me, this was at line 408 after the property footerRight was declared.
#+end_src

However, line 408 for me (~footerRight~) was in PageSettings. Perhaps it's a typo?
We'll see...

I'll just collect notes as I go and then (once I get everything working), create a pull request with any suggested changes.

#+begin_src
    Modify ext-tabs.js, replacing the code for the saveAsPDF API with the code that follows:
#+end_src

Is this parent/ext-tabs.js or child/ext-tabs.js?

The code snippet for ~saveAsPDF~ should have a trailing comma.


** 2023-11-25
*** Tightening up
When I initially ran through setup, I used Ubuntu Server 23.10 because it was the latest version available. In retrospect, things would have been easier on Ubuntu Server 22.04 because NVIDIA officially supports ~cuda~ and ~nvidia-container-toolkit~ on 22.04. I was able to get everything working, but since ~pytorch~, ~cuda~, and ~nvidia~ driver dependencies are notoriously tricky for new users to get right, I may want to go back and reinstall on 22.04.

To do so, I will need to:
- Create a new installation media with Ubuntu Server 22.04
- Install Ubuntu Server 22.04
- Redo setup

This will also give me the chance to verify that everything is working as expected, as it's easy when working with ansible to run a step and then delete it from the setup script so that future installations don't work as intended, or to do things out of order as you're the setup script is being written.

Also, I am installing more NVIDIA software to the base system than is strictly necessary, given that I plan to run all the programs in docker containers. I do not need CUDA installed on the base system -- only the NVIDIA driver(s). CUDA is included INSIDE the docker images / containers.

I flashed a new image onto the thumb drive:
#+begin_src bash
dd bs=4M if=./ubuntu-22.04.3-live-server-amd64.iso of=/dev/sda status=progress oflag=sync
#+end_src

I updated the README.md to explain the NVIDIA dependencies in more detail (and what NVIDIA Container Toolkit does).

At the moment I don't want to actually go through with reinstalling everything, because I would rather play with some AI applications (like Control Net). So I will worry about reinstalling on 22.04 later.

** 2023-11-02
*** Initial Setup

After experimenting with the nix package manager, nixos, and nix flakes, I decided to start over. I have a powerful machine that I'd like to run AI workloads on, and I'd like to enable my coworkers to use the machine as well. Nix's promises of reproducability and auditibility seemed like a good choice for this, but ultimately I found it difficult to install the exact set of packages I needed for each project, and I also couldn't imagine walking my coworkers through the (nix) process of setting up their custom environments. Today I will try to set things up with ansible and ubuntu server instead, because I suspect this will be a much smoother learning curve (while still giving me the benefits of having documentation of what kinds of changes I've made to the base system.)

I downloaded an iso of Ubuntu Server 23.10 and created an installation drive. ~udev~ made my usb drive available at ~/dev/sda~ :

#+begin_src bash
dd bs=4M if=./ubuntu-23.10-live-server-amd64.iso of=/dev/sda status=progress oflag=sync
#+end_src

I went through the initial setup and chose the "normal" options, rather than the "minimalized" option.

I gradually wrote setup.yml to include all the packages I needed. The system I had in mind was this:

- Install nvidia/cuda drivers that are appropriate for the hardware (two 4090's) and OS (ubuntu server 23.10) or more specifically the kernel version (~$(uname -r)~).
- Install docker and nvidia-container-runtime, so that I could run cuda-accelerated containers
- Write a dockerfile (or find an existing image) that I could use to run jupyter lab with all of the dependencies I needed for fastai's fastbook project.

After this, I could repeat the last step for any other project. As long as I kept all the configuration of the first two layers in ansible, I figured I'd have about as reproducable a setup as I could hope for. It's not quite as "locked in" as a nix solution would be -- but in truth I couldn't get a nix solution up and running reliably in a few days, and I didn't think spending a few more days on it was a good idea given that I thought I could set things up on ubuntu in a matter of hours.


*** NVIDIA Container Toolkit

The [[https://github.com/NVIDIA/nvidia-container-toolkit][NVIDIA Container Toolkit]] allows users to build and run GPU accelerated containers.

I created a test docker image and python script that will confirm that cuda and pytorch are working well together inside the docker container.

*** Documentation

I am _probably_ the only person who is going to use this setup, but one of my goals is to help other developers (my coworkers and anyone else) with these kinds of problems. I learned a little bit about all of the dependencies by doing this exercise, and figure that it may be valuable to write things down.
